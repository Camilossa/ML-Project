# üéì Student Performance Prediction - End-to-End ML Project

## üìã Descripci√≥n del Proyecto

Este es un proyecto completo de Machine Learning que predice el rendimiento acad√©mico de estudiantes en matem√°ticas bas√°ndose en diversas caracter√≠sticas demogr√°ficas y educativas. El proyecto implementa un pipeline completo desde la ingesta de datos hasta el despliegue de una aplicaci√≥n web.

### üéØ Objetivo
Predecir la puntuaci√≥n en matem√°ticas de un estudiante utilizando:
- Puntuaciones en lectura y escritura
- Caracter√≠sticas demogr√°ficas (g√©nero, etnia)
- Nivel educativo de los padres
- Tipo de almuerzo
- Curso de preparaci√≥n para ex√°menes

## üèóÔ∏è Arquitectura del Proyecto

```
üì¶ ML-project/
‚îú‚îÄ‚îÄ üìÅ src/                          # C√≥digo fuente principal
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ components/               # Componentes del pipeline ML
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ data_ingestion.py     # Carga y divisi√≥n de datos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ data_transformation.py # Preprocesamiento de datos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ model_trainer.py      # Entrenamiento de modelos
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ pipeline/                 # Pipelines de predicci√≥n
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ predict_pipeline.py   # Pipeline de predicci√≥n
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ train_pipeline.py     # Pipeline de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ exception.py              # Manejo de excepciones
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ logger.py                 # Sistema de logging
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ utils.py                  # Funciones utilitarias
‚îú‚îÄ‚îÄ üìÅ templates/                    # Templates HTML para Flask
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ home.html                 # P√°gina de predicci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ index.html                # P√°gina principal
‚îú‚îÄ‚îÄ üìÅ notebook/                     # An√°lisis exploratorio (EDA)
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 1. EDA STUDENT PERFORMANCE.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ data/
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ stud.csv              # Dataset original
‚îú‚îÄ‚îÄ üìÅ artifacts/                    # Modelos y objetos entrenados
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ model.pkl                 # Modelo entrenado
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ preprocessor.pkl          # Objeto de preprocesamiento
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ train.csv                 # Datos de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test.csv                  # Datos de prueba
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ data.csv                  # Datos originales procesados
‚îú‚îÄ‚îÄ üìÑ app.py                        # Aplicaci√≥n Flask
‚îú‚îÄ‚îÄ üìÑ requirements.txt              # Dependencias
‚îî‚îÄ‚îÄ üìÑ setup.py                      # Configuraci√≥n del paquete
```

## üîÑ Flujo de Datos y Arquitectura

```mermaid
graph TD
    A[üìä Dataset Original] --> B[üîÑ Data Ingestion]
    B --> C[üìù Train/Test Split]
    C --> D[üõ†Ô∏è Data Transformation]
    D --> E[ü§ñ Model Training]
    E --> F[üíæ Model Storage]
    F --> G[üåê Flask Web App]
    G --> H[üì± User Interface]
    
    B --> I[üìÅ artifacts/data.csv]
    C --> J[üìÅ artifacts/train.csv]
    C --> K[üìÅ artifacts/test.csv]
    D --> L[üìÅ artifacts/preprocessor.pkl]
    E --> M[üìÅ artifacts/model.pkl]
    
    style A fill:#e1f5fe
    style G fill:#c8e6c9
    style H fill:#fff3e0
```

## üß© Componentes Detallados

### 1. üì• Data Ingestion (`data_ingestion.py`)

**Funci√≥n**: Carga y prepara los datos iniciales

```python
# Funcionalidades principales:
‚úÖ Carga el dataset desde notebook/data/stud.csv
‚úÖ Divide los datos en conjuntos de entrenamiento y prueba (80/20)
‚úÖ Guarda los archivos en la carpeta artifacts/
‚úÖ Retorna las rutas de los archivos procesados
```

**Flujo de trabajo**:
1. Lee el archivo CSV original
2. Crea la estructura de directorios `artifacts/`
3. Divide los datos usando `train_test_split`
4. Guarda: `data.csv`, `train.csv`, `test.csv`

### 2. üîß Data Transformation (`data_transformation.py`)

**Funci√≥n**: Preprocesa y transforma los datos para el modelo

**Variables del Dataset**:
- **Num√©ricas**: `writing_score`, `reading_score`
- **Categ√≥ricas**: `gender`, `race_ethnicity`, `parental_level_of_education`, `lunch`, `test_preparation_course`
- **Target**: `math_score` (variable a predecir)

**Pipelines de Transformaci√≥n**:

```python
# Pipeline Num√©rico:
üîπ SimpleImputer(strategy="median")     # Rellena valores faltantes
üîπ StandardScaler()                     # Normalizaci√≥n (Œº=0, œÉ=1)

# Pipeline Categ√≥rico:
üîπ SimpleImputer(strategy="most_frequent")  # Rellena con moda
üîπ OneHotEncoder()                          # Codificaci√≥n dummy
üîπ StandardScaler(with_mean=False)          # Normalizaci√≥n sin centrar
```

### 3. ü§ñ Model Trainer (`model_trainer.py`)

**Funci√≥n**: Entrena y eval√∫a m√∫ltiples modelos de ML

**Modelos Implementados**:
- üå≥ Random Forest Regressor
- üå≤ Decision Tree Regressor  
- üìà Gradient Boosting Regressor
- üìè Linear Regression
- üöÄ XGBoost Regressor
- üê± CatBoost Regressor
- üîÑ AdaBoost Regressor

**Proceso de Entrenamiento**:
1. Separaci√≥n de features (X) y target (y)
2. Entrenamiento con GridSearchCV para optimizaci√≥n de hiperpar√°metros
3. Evaluaci√≥n usando R¬≤ Score
4. Selecci√≥n del mejor modelo
5. Guardado del modelo en `artifacts/model.pkl`

### 4. üîÆ Prediction Pipeline (`predict_pipeline.py`)

**Funci√≥n**: Maneja las predicciones en producci√≥n

**Componentes**:
- **PredictPipeline**: Carga modelo y preprocessor, realiza predicciones
- **CustomData**: Estructura los datos de entrada del usuario

### 5. üåê Flask Web Application (`app.py`)

**Funci√≥n**: Interfaz web para interactuar con el modelo

**Rutas**:
- `/`: P√°gina principal
- `/predictdata`: Formulario de predicci√≥n (GET/POST)

## üöÄ Instalaci√≥n y Configuraci√≥n

### Prerrequisitos
```bash
Python 3.11.4+
pip
```

### 1. Clonar el Repositorio
```bash
git clone https://github.com/Camilossa/ML-Project.git
cd ML-project
```

### 2. Crear Entorno Virtual
```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3. Instalar Dependencias
```bash
pip install -r requirements.txt
```

### 4. Ejecutar el Pipeline de Entrenamiento
```bash
python3 src/components/data_ingestion.py
```

### 5. Lanzar la Aplicaci√≥n Web
```bash
python3 app.py
```

La aplicaci√≥n estar√° disponible en: `http://localhost:8080`

## üìä Dataset

**Fuente**: Student Performance Dataset
**Registros**: ~1000 estudiantes
**Features**: 8 variables (5 categ√≥ricas, 2 num√©ricas, 1 target)

### Descripci√≥n de Variables:

| Variable | Tipo | Descripci√≥n |
|----------|------|-------------|
| `gender` | Categ√≥rica | G√©nero del estudiante (male/female) |
| `race_ethnicity` | Categ√≥rica | Grupo √©tnico (group A-E) |
| `parental_level_of_education` | Categ√≥rica | Nivel educativo de los padres |
| `lunch` | Categ√≥rica | Tipo de almuerzo (standard/free or reduced) |
| `test_preparation_course` | Categ√≥rica | Curso de preparaci√≥n (completed/none) |
| `reading_score` | Num√©rica | Puntuaci√≥n en lectura (0-100) |
| `writing_score` | Num√©rica | Puntuaci√≥n en escritura (0-100) |
| `math_score` | Num√©rica | **Puntuaci√≥n en matem√°ticas (TARGET)** |

## üîß Tecnolog√≠as Utilizadas

### Machine Learning
- **scikit-learn**: Modelos base y preprocesamiento
- **XGBoost**: Gradient boosting optimizado
- **CatBoost**: Manejo especializado de variables categ√≥ricas
- **pandas/numpy**: Manipulaci√≥n de datos

### Web Framework
- **Flask**: Framework web para la aplicaci√≥n
- **HTML/CSS**: Interface de usuario

### DevOps y Utilidades
- **pickle/dill**: Serializaci√≥n de modelos
- **logging**: Sistema de logs
- **dataclasses**: Estructuras de configuraci√≥n

## üìà Rendimiento del Modelo

El sistema eval√∫a autom√°ticamente m√∫ltiples modelos y selecciona el mejor bas√°ndose en el R¬≤ Score. Los modelos t√≠picamente logran:

- **R¬≤ Score**: 0.85-0.92
- **RMSE**: 5-8 puntos
- **Tiempo de entrenamiento**: 2-5 minutos

## üåê Uso de la Aplicaci√≥n Web

### Interfaz de Usuario

1. **P√°gina Principal** (`/`): Bienvenida al sistema
2. **Formulario de Predicci√≥n** (`/predictdata`):
   - Seleccionar caracter√≠sticas del estudiante
   - Ingresar puntuaciones de lectura y escritura
   - Obtener predicci√≥n de matem√°ticas

### Ejemplo de Uso:
```
Entrada:
- G√©nero: Female
- Etnia: Group C
- Educaci√≥n Padres: Bachelor's degree
- Almuerzo: Standard
- Curso Preparaci√≥n: Completed
- Lectura: 85
- Escritura: 88

Salida: Predicci√≥n Matem√°ticas: 87.3
```

## üîÑ Pipeline de Desarrollo

### Para Entrenar un Nuevo Modelo:
```bash
# 1. Actualizar datos en notebook/data/stud.csv
# 2. Ejecutar pipeline completo
python3 src/components/data_ingestion.py

# 3. Los artefactos se generan autom√°ticamente en artifacts/
```

### Para Hacer Predicciones:
```python
from src.pipeline.predict_pipeline import PredictPipeline, CustomData

# Crear datos de ejemplo
data = CustomData(
    gender="female",
    race_ethnicity="group C",
    parental_level_of_education="bachelor's degree",
    lunch="standard",
    test_preparation_course="completed",
    reading_score=85,
    writing_score=88
)

# Realizar predicci√≥n
pipeline = PredictPipeline()
result = pipeline.predict(data.get_data_as_data_frame())
print(f"Predicci√≥n: {result[0]}")
```

## üìù Logging y Monitoreo

El sistema incluye logging comprehensivo:
- **Ubicaci√≥n**: `logs/` (generados autom√°ticamente)
- **Formato**: Timestamp, l√≠nea, nivel, mensaje
- **Cobertura**: Todos los componentes del pipeline

## üö® Manejo de Errores

Sistema robusto de manejo de excepciones:
- **CustomException**: Excepciones personalizadas con contexto
- **Error tracking**: Archivo, l√≠nea, mensaje detallado
- **Graceful fallbacks**: El sistema contin√∫a funcionando ante errores menores

## ü§ù Contribuciones

Para contribuir al proyecto:

1. Fork el repositorio
2. Crea una rama feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -m 'A√±adir nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver archivo `LICENSE` para m√°s detalles.

## üë®‚Äçüíª Autor

**Juan Camilo Ossa Giraldo**
- Email: ossagiraldojuancamilo@gmail.com
- GitHub: [@Camilossa](https://github.com/Camilossa)

## üìå Notas Adicionales

> **Nota Importante**: La carpeta `notebook/` contiene el an√°lisis exploratorio de datos (EDA) que sirvi√≥ como base para desarrollar el c√≥digo modular. Es una referencia de c√≥mo se desarroll√≥ la l√≥gica que luego se implement√≥ en los componentes del pipeline.

---

‚≠ê **Si este proyecto te fue √∫til, no olvides darle una estrella en GitHub!**